{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanketrk/Clustering-of-Network-Complaint-Data/blob/master/2_Hands_on_Using_ChatGPT_or_Gemini_with_Python_and_LangChain_for_real_world_tasks_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ChatGPT or Gemini with Python and LangChain for real-world tasks\n",
        "\n",
        "In this notebook you will use ChatGPT or Gemini and LangChain to solve and learn about:\n",
        "\n",
        "- Langchain chains\n",
        "- Memory and conversation chains\n",
        "\n",
        "- Exercise 1: Review Analysis and Response\n",
        "- Exercise 2: Paper Analysis and Summarization\n",
        "\n",
        "\n",
        "- Bonus: Build a text-based chatbot\n",
        "\n",
        "___[Created By: Dipanjan (DJ)](https://www.linkedin.com/in/dipanjans/)___"
      ],
      "metadata": {
        "id": "XTzBUFWQ-OWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI and LangChain dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2evPp14fy258",
        "outputId": "135ca39d-1586-461d-8b55-cd9f8ced0640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai\n",
            "  Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.28 (from langchain)\n",
            "  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.31 (from langchain)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.27-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Collecting openai<2.0.0,>=1.10.0 (from langchain-openai)\n",
            "  Downloading openai-1.14.1-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain-openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.7.1)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.31->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, h11, typing-inspect, tiktoken, marshmallow, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-text-splitters, langchain-openai, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-openai-0.0.8 langchain-text-splitters-0.0.1 langsmith-0.1.27 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.14.1 orjson-3.9.15 packaging-23.2 tiktoken-0.6.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional: Install LangChain Google Gemini Dependency\n",
        "\n",
        "Google Gemini API is free (till now). You can get a key [here](https://aistudio.google.com/app/apikey), just need to sign in with your google account. Gemini may not be available fully in EU."
      ],
      "metadata": {
        "id": "s3XbTsMsuEn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "id": "s7rII_GgugOW",
        "outputId": "506672bf-218a-4f0f-d7c0-3531a1babdd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-0.0.11-py3-none-any.whl (28 kB)\n",
            "Collecting google-generativeai<0.5.0,>=0.4.1 (from langchain-google-genai)\n",
            "  Downloading google_generativeai-0.4.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.1.32)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (2.11.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (2.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.23.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (0.1.27)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (23.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain-google-genai) (8.2.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain-google-genai) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1->langchain-google-genai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1->langchain-google-genai) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain-google-genai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain-google-genai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain-google-genai) (2024.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.62.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->langchain-google-genai) (0.5.1)\n",
            "Installing collected packages: google-generativeai, langchain-google-genai\n",
            "  Attempting uninstall: google-generativeai\n",
            "    Found existing installation: google-generativeai 0.3.2\n",
            "    Uninstalling google-generativeai-0.3.2:\n",
            "      Successfully uninstalled google-generativeai-0.3.2\n",
            "Successfully installed google-generativeai-0.4.1 langchain-google-genai-0.0.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "eeea0c20a51549c5ab91eb955044342f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load OpenAI API Credentials\n",
        "\n",
        "Here we load it from a file so we don't explore the credentials on the internet by mistake"
      ],
      "metadata": {
        "id": "CiwGjVWK4q6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "5e1HqI56y7t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryheOZuXxa41"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "with open('chatgpt_api_credentials.yml', 'r') as file:\n",
        "    api_creds = yaml.safe_load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_creds.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZs7ts6NzADJ",
        "outputId": "f6520cab-6356-45d2-ca65-bbca6e315e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['openai_key'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = api_creds['openai_key']"
      ],
      "metadata": {
        "id": "kDe44J0N0NcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Load Gemini API credentials\n",
        "\n",
        "Run this section only if you are using Google Gemini"
      ],
      "metadata": {
        "id": "LS7koM2emZ_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "with open('gemini_key.yml', 'r') as file:\n",
        "    api_creds = yaml.safe_load(file)\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_creds['gemini_key']"
      ],
      "metadata": {
        "id": "nxJAcO1MmhRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Necessary Dependencies and ChatGPT LLM"
      ],
      "metadata": {
        "id": "VDWhgxCy5bA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "9GYhyRFRuJXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.0)"
      ],
      "metadata": {
        "id": "mY2bapqfuWq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Load Google Gemini LLM\n",
        "\n",
        "Only run the below cell if you don't want to use ChatGPT and want to use Google Gemini"
      ],
      "metadata": {
        "id": "6CHIZflB3X6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_model = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)"
      ],
      "metadata": {
        "id": "YGXBXnHd3g6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's look at how to create a basic chain"
      ],
      "metadata": {
        "id": "qJrIiENwxMx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"tell me a joke about {topic}\"\n",
        "prompt = ChatPromptTemplate.from_template(PROMPT)\n",
        "chain = (\n",
        "         prompt\n",
        "         |\n",
        "         model\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"topic\": \"bears\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Evdag0KuOXo",
        "outputId": "7e8a3342-a96d-4499-8332-6f9bfb17b2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the bear break up with his girlfriend? \n",
            "\n",
            "Because he couldn't bear the relationship any longer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same code with gemini\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "chain = (\n",
        "         prompt\n",
        "         |\n",
        "         gemini_model\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"topic\": \"bears\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN-2Plhs3oTX",
        "outputId": "5a276c0d-d417-4e7c-b46c-bc81ef6746d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you call a bear with no teeth?\n",
            "\n",
            "A gummy bear.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note: Replace model with gemini_model in any of the code below if you want to use Google Gemini instead of ChatGPT"
      ],
      "metadata": {
        "id": "na0SulKs4hXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# can be used on multiple prompts also\n",
        "topics = [{'topic': 'AI'}, {'topic': 'Statistics'}]\n",
        "responses = chain.map().invoke(topics)"
      ],
      "metadata": {
        "id": "Tp84pgKXvKuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for response in responses:\n",
        "  print(response.content)\n",
        "  print('-----')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QePjecVqviA3",
        "outputId": "fb5b946c-d8a8-43e9-addc-5adcf03704c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the AI cross the road?\n",
            "\n",
            "To get to the other algorithm.\n",
            "-----\n",
            "\n",
            "\n",
            "Why did the statistician put butter on his head?\n",
            "\n",
            "Because he wanted to make a point spread.\n",
            "-----\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic chains are ad-hoc - No conversation history!"
      ],
      "metadata": {
        "id": "jLH8WUaumNMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"{query}\")\n",
        "basic_chain = (\n",
        "               prompt\n",
        "               |\n",
        "              model\n",
        ")"
      ],
      "metadata": {
        "id": "_LZSwWNCx2EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = basic_chain.invoke({\"query\" : 'What are the first four colors of the rainbow?'})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhvUjj6Vx8QV",
        "outputId": "07b5d421-9132-4fc3-bf4d-9831c64fd8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first four colors of the rainbow are red, orange, yellow, and green.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = basic_chain.invoke({\"query\" : 'And the other three?'})\n",
        "print(response.content) # gives a totally random response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9nLc6PnySWh",
        "outputId": "70de6140-a3ab-4982-b8f7-e08cb16cb822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The other three are: \n",
            "\n",
            "- The Father\n",
            "- The Son\n",
            "- The Holy Spirit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's learn how to add memory to build a conversation chain"
      ],
      "metadata": {
        "id": "Y6NeOZe6mZ8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
      ],
      "metadata": {
        "id": "AEQ7Ame0yWwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create prompt template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Act as a helpful AI Assistant\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ],
      "metadata": {
        "id": "yQliFt8lzdRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({}) # shows the conversation history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihya5We_RPdH",
        "outputId": "e3adbb72-1627-4992-b2d8-ac873dc84e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': []}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "# creates the conversation chain\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(memory.load_memory_variables)\n",
        "        |\n",
        "        itemgetter(\"history\")\n",
        "    )\n",
        "    |\n",
        "    prompt\n",
        "    |\n",
        "    model\n",
        ")"
      ],
      "metadata": {
        "id": "66AQdRNaRYYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = {'input': 'What are the first four colors of a rainbow'}\n",
        "response = chain.invoke(user_input)\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wfgXhIU-U05R",
        "outputId": "e5832dac-00e7-4992-bcd2-e282d8ada63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The first four colors of a rainbow are red, orange, yellow, and green.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(user_input, {\"output\": response.content})\n",
        "memory.load_memory_variables({}) # remembers the conversation history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83bpMrWKVG_H",
        "outputId": "ad2bcc75-ef11-4d45-b42c-b45901097900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What are the first four colors of a rainbow'),\n",
              "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = {'input': 'And the last 3?'}\n",
        "response = chain.invoke(user_input) # uses history of the past conversation to give a better response\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7w2ENCJJVjHs",
        "outputId": "4ed3556d-a569-4b56-efa4-8d5671ac34cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The last three colors of a rainbow are blue, indigo, and violet.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context(user_input, {\"output\": response.content})\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzGwOdPuVfwy",
        "outputId": "d635125b-cfe3-4914-f948-545806de211a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What are the first four colors of a rainbow'),\n",
              "  AIMessage(content='The first four colors of a rainbow are red, orange, yellow, and green.'),\n",
              "  HumanMessage(content='And the last 3?'),\n",
              "  AIMessage(content='The last three colors of a rainbow are blue, indigo, and violet.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Review Analysis and Response\n",
        "\n",
        "For each review get ChatGPT to do the following and use regular llm chains:\n",
        "\n",
        "  - Summarize the review below, delimited by triple\n",
        "  backticks. The summary should be at most 3 lines.\n",
        "  - Highlight both the positives and negatives\n",
        "  - Display the overall sentiment of the review (positive, negative, neutral)\n",
        "  - Display a list of 3 - 5 emotions expressed by the customer in the review\n",
        "  - If the sentiment is positive or neutral write an email and thank them for the review\n",
        "  - If the sentiment is negative apologize and write an email with an appropriate response\n",
        "\n",
        "\n",
        "Remember you can pass the list of documents using the `map().invoke(...)` function with your `chain`"
      ],
      "metadata": {
        "id": "AeDkpvGDhMGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "    f\"\"\"\n",
        "    Just received the Bluetooth speaker I ordered for beach outings, and it's fantastic.\n",
        "    The sound quality is impressively clear with just the right amount of bass.\n",
        "    It's also waterproof, which tested true during a recent splashing incident.\n",
        "    Though it's compact, the volume can really fill the space.\n",
        "    The price was a bargain for such high-quality sound.\n",
        "    Shipping was also on point, arriving two days early in secure packaging.\n",
        "    \"\"\",\n",
        "    f\"\"\"\n",
        "    Purchased a new gaming keyboard because of its rave reviews about responsiveness and backlighting.\n",
        "    It hasn't disappointed. The keys have a satisfying click and the LED colors are vibrant,\n",
        "    enhancing my gaming experience significantly. Price-wise, it's quite competitive,\n",
        "    and I feel like I got a good deal. The delivery was swift, and it came well-protected,\n",
        "    ensuring no damage during transport.\n",
        "    \"\"\",\n",
        "    f\"\"\"\n",
        "    Ordered a set of wireless earbuds for running, and they've been a letdown.\n",
        "    The sound constantly cuts out, and the fit is uncomfortable after only a few minutes of use.\n",
        "    They advertised a 12-hour battery life, but I'm barely getting four hours.\n",
        "    Considering the cost, I expected better quality and performance.\n",
        "    They did arrive on time, but the positives end there. I'm already looking into a return.\n",
        "    \"\"\",\n",
        "    f\"\"\"\n",
        "    The tablet stand I bought was touted as being sturdy and adjustable,\n",
        "    but it's anything but. It wobbles with the slightest touch,\n",
        "    and the angles are not holding up as promised. It feels like a breeze could knock it over.\n",
        "    It was also pricier than others I've seen, which adds to the disappointment.\n",
        "    It did arrive promptly, but what's the use if the product doesn't meet basic expectations?\n",
        "    \"\"\",\n",
        "    f\"\"\"\n",
        "    Needed a new kitchen blender, but this model has been a nightmare.\n",
        "    It's supposed to handle various foods, but it struggles with anything tougher than cooked vegetables.\n",
        "    It's also incredibly noisy, and the 'easy-clean' feature is a joke; food gets stuck under the blades constantly.\n",
        "    I thought the brand meant quality, but this product has proven me wrong.\n",
        "    Plus, it arrived three days late. Definitely not worth the expense.\n",
        "    \"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "hRbBZB57hT0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "            Act as a product review analyst.\n",
        "            Your task is to perform the following tasks:\n",
        "\n",
        "            - Summarize the review below, delimited by triple\n",
        "            backticks. The summary should be at most 3 lines.\n",
        "            - Highlight both the positives and negatives\n",
        "            - Display the overall sentiment of the review (positive, negative, neutral)\n",
        "            - Display a list of 3 - 5 emotions expressed by the customer in the review\n",
        "            - If the sentiment is positive or neutral write an email and thank them for the review\n",
        "            - If the sentiment is negative apologize and write an email with an appropriate response\n",
        "\n",
        "            ```{review}```\n",
        "\"\"\"\n",
        "\n",
        "reviews_formatted = [{'review': review} for review in reviews]\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
        "llm_chain = (\n",
        "    prompt_template\n",
        "    |\n",
        "    model\n",
        ")"
      ],
      "metadata": {
        "id": "mlkR6kQfhdMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = llm_chain.map().invoke(reviews_formatted)"
      ],
      "metadata": {
        "id": "d_oo3lVVFckF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for result in results:\n",
        "  print(result.content)\n",
        "  print('-----')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWlI8SYRGUM7",
        "outputId": "86ab0bd7-0759-4d58-a544-f22b7f911e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: The customer is extremely satisfied with the Bluetooth speaker, praising its sound quality, waterproof feature, compact size, volume, price, and fast shipping.\n",
            "\n",
            "Positives: Clear sound quality, just the right amount of bass, waterproof, compact size, volume, price, fast shipping.\n",
            "\n",
            "Negatives: None mentioned.\n",
            "\n",
            "Overall sentiment: Positive\n",
            "\n",
            "Emotions: Satisfaction, excitement, happiness\n",
            "\n",
            "Email response:\n",
            "Subject: Thank you for your positive review of our Bluetooth speaker!\n",
            "\n",
            "Dear [Customer's Name],\n",
            "\n",
            "Thank you so much for taking the time to share your positive feedback about the Bluetooth speaker you purchased for beach outings. We are thrilled to hear that you are enjoying the sound quality, waterproof feature, and overall performance of the speaker. Your satisfaction is our top priority, and we are delighted that we could meet your expectations. If you have any further feedback or need assistance, please do not hesitate to reach out to us.\n",
            "\n",
            "Thank you once again for choosing our product!\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "[Your Title]\n",
            "[Company Name]\n",
            "-----\n",
            "\n",
            "\n",
            "- The review highlights the gaming keyboard's responsiveness, satisfying key clicks, vibrant LED colors, competitive pricing, and swift delivery.\n",
            "- Positives: Satisfying key clicks, vibrant LED colors, competitive pricing, swift delivery, well-protected packaging.\n",
            "- Negatives: None mentioned.\n",
            "- Overall sentiment: Positive\n",
            "- Emotions: Satisfaction, excitement, gratitude\n",
            "\n",
            "Dear [Customer],\n",
            "\n",
            "Thank you for your positive review of our gaming keyboard! We are thrilled to hear that it has enhanced your gaming experience significantly. We appreciate your feedback and are glad that you are satisfied with the product and our service. If you have any questions or need further assistance, please feel free to reach out to us.\n",
            "\n",
            "Thank you once again for choosing our product.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "-----\n",
            "\n",
            "\n",
            "- Summary: The customer ordered wireless earbuds for running but found them to be a letdown due to sound cutting out, uncomfortable fit, and shorter battery life than advertised.\n",
            "- Positives: Earbuds arrived on time.\n",
            "- Negatives: Sound cutting out, uncomfortable fit, shorter battery life, expected better quality for the cost.\n",
            "- Overall sentiment: Negative\n",
            "- Emotions: Disappointment, frustration, dissatisfaction\n",
            "- Email response: \n",
            "  Subject: Apology for Your Recent Experience with Our Wireless Earbuds\n",
            "  Dear [Customer's Name],\n",
            "  We are truly sorry to hear about your disappointing experience with our wireless earbuds. We understand your frustration and would like to offer our sincerest apologies. Please let us know how we can make this right for you.\n",
            "  Sincerely, [Your Name]\n",
            "-----\n",
            "\n",
            "\n",
            "Summary: The review highlights the tablet stand as not sturdy or adjustable, wobbling easily and not holding angles well. The customer is disappointed in the product's quality and price.\n",
            "\n",
            "Positives: Prompt delivery\n",
            "Negatives: Not sturdy or adjustable, wobbles easily, angles not holding up, pricey\n",
            "\n",
            "Overall sentiment: Negative\n",
            "\n",
            "Emotions: Disappointment, frustration, dissatisfaction\n",
            "\n",
            "Dear customer,\n",
            "\n",
            "We are truly sorry to hear about your experience with the tablet stand you purchased. We apologize for the inconvenience caused and would like to offer you a full refund or a replacement product. Your feedback is valuable to us, and we will work on improving our product quality in the future.\n",
            "\n",
            "Sincerely,\n",
            "[Your Name]\n",
            "-----\n",
            "\n",
            "\n",
            "- The review highlights that the blender struggles with tougher foods, is noisy, and the 'easy-clean' feature is ineffective.\n",
            "- The customer is disappointed with the product's quality and the late delivery.\n",
            "- Overall sentiment: Negative\n",
            "\n",
            "Emotions expressed: disappointment, frustration, skepticism\n",
            "\n",
            "I'm sorry to hear about your negative experience with our blender. We apologize for the inconvenience caused by the late delivery and the issues you encountered with the product. Please reach out to our customer service team for assistance.\n",
            "-----\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Paper Analysis and Summarization\n",
        "\n",
        "- Act as a Artificial Intelligence Expert.\n",
        "Transform this research paper abstract in triple backticks\n",
        "into a short concise version of maximum 10 lines for your audience.\n",
        "\n",
        "- Act as a Artificial Intelligence Expert.\n",
        "Transform this research paper abstract in triple backticks\n",
        "into an executive summary for a healthcare company.\n",
        "Have bullet points for pros and cons of ethics in Generative AI as mentioned in the paper.\n",
        "\n",
        "- Act as a Artificial Intelligence Expert.\n",
        "Transform this research paper abstract in triple backticks\n",
        "into an executive summary for a generative AI company solving healthcare problems.\n",
        "Have bullet points for key points mentioned for\n",
        "Generative AI for text, images and structured data based healthcare\n",
        "\n",
        "Use Conversation Chains with `ConversationBufferMemory` and modify ChatGPT behavior with System prompts as necessary to do the above 3"
      ],
      "metadata": {
        "id": "eEtB1IOimA0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paper_abstract = f\"\"\"\n",
        "The widespread use of ChatGPT and other emerging technology powered by generative\n",
        "artificial intelligence (AI) has drawn much attention to potential ethical issues, especially in\n",
        "high-stakes applications such as healthcare.1–3 However, less clear is how to resolve such\n",
        "issues beyond following guidelines and regulations that are still under discussion and\n",
        "development. On the other hand, other types of generative AI have been used to synthesize\n",
        "images and other types of data for research and practical purposes, which have resolved some\n",
        "ethical issues and exposed other ethical issues,4,5 but such technology is less often the focus\n",
        "of ongoing ethical discussions. Here we highlight gaps in current ethical discussions of\n",
        "generative AI via a systematic scoping review of relevant existing research in healthcare, and\n",
        "reduce the gaps by proposing an ethics checklist for comprehensive assessment and\n",
        "transparent documentation of ethical discussions in generative AI development. While the\n",
        "checklist can be readily integrated into the current peer review and publication system to\n",
        "enhance generative AI research, it may also be used in broader settings to disclose ethicsrelated considerations in generative AI-powered products (or real-life applications of such\n",
        "products) to help users establish reasonable trust in their capabilities.\n",
        "\n",
        "Current ethical discussions on generative AI in healthcare\n",
        "We conducted a systematic scoping review to analyse current ethical discussions on\n",
        "generative AI in healthcare. Our search in four major academic research databases for\n",
        "relevant publications from January 2013 to July 2023 yielded 2859 articles (see Methods for\n",
        "detailed search strategy and Supplementary Figure S1 for the PRISMA flow diagram), of\n",
        "which 193 articles were included for analysis based on application data modality (text, image,\n",
        "or structured data), ethical issues discussed, generative AI involved, and whether generative\n",
        "AI causes or offers technical solutions for issues raised.\n",
        "\n",
        "Generative AI for text data-based healthcare\n",
        "Forty-one of the 193 articles discussed ethical considerations pertaining to generative AI\n",
        "applications for text data, with 20 articles describing methodological developments or\n",
        "applications of generative AI and the other 21 articles describing review-type works on this\n",
        "topic. Although some of these review-type articles used the general term “generative AI”, the\n",
        "main body and supporting evidence focused on LLMs. Twenty-nine articles had in-depth\n",
        "discussions on ethical issues, whereas the other 12 articles only briefly touched on some\n",
        "ethical aspects.\n",
        "Among the 41 articles, 29 articles focused on discussing ethical issues caused by LLMs (and\n",
        "specifically by GPT in 16 of the articles), covering a wide range of application scenarios and\n",
        "considered the application of all 10 ethical principles identified in the review (see Figure 1),\n",
        "as well as other less discussed concerns such as human-AI interaction, and the rights of\n",
        "LLMs to be considered as co-authors in scientific papers. One paper only commented briefly\n",
        "on the need for ethical considerations in LLMs and is summarised in the “Others” category.\n",
        "Although all ethical principles are equally important, some are discussed more often than\n",
        "others, e.g., non-maleficence (also referred to in the literature as ‘benevolence’), equity, and\n",
        "privacy.\n",
        "Fifteen of the 41 articles aimed to resolve some existing ethical issues (for example,\n",
        "confidentiality of medical data) by using LLMs and other generative AI (e.g., GAN,\n",
        "autoencoder or diffusion), such as, to reduce privacy concerns by generating synthetic\n",
        "medical text, to reduce disparity by providing accessible services and assistance, to detect\n",
        "health-related misinformation, to generate trusted content, and to improve accountability or\n",
        "transparency over existing approaches. While most articles focused on either identifying\n",
        "ethical issues caused by generative AI or proposing generative AI-based solutions, three\n",
        "articles discussed both to provide a more balanced perspective.\n",
        "\n",
        "Generative AI for image and structured data-based healthcare\n",
        "Unlike the diverse application scenarios of generative AI based on text data, for image and\n",
        "structured data, this use of generative AI focuses on data synthesis and encryption. Hence the\n",
        "majority of articles discussed the methodological developments of generative AI as giving\n",
        "rise to a more distinctive and focused set of ethical issues.\n",
        "5\n",
        "Notably, of the 98 articles on image data and 58 articles on structured data, more than half\n",
        "(n=63 for image data and n=33 for structured data) only mentioned ethical considerations as a\n",
        "brief motivation for methodological developments or as a general discussion point. The rest\n",
        "included more in-depth discussions or evaluations of ethical issues. Among these 155 articles\n",
        "(as one article covered multiple modalities), 11 articles were review-type work, where 10\n",
        "articles reviewed methods that mentioned one or two ethical perspectives, and only one\n",
        "article24 discussed detailed ethical concerns on generative AI applications.\n",
        "Resolving privacy issues was the main aim of articles for these two data modalities (n=74 for\n",
        "image data and n=50 for structured data; see Figure 1), predominantly by generating synthetic\n",
        "data using GAN. Eight articles on image data and 9 articles on structured data used\n",
        "generative AI to reduce bias, e.g., by synthesizing data for under-represented subgroups in\n",
        "existing databases. For both data modalities, we did not see explicit discussions on resolving\n",
        "autonomy, integrity, or morality issues using generative AI, and for structured data the articles\n",
        "additionally lacked discussions on trust or transparency.\n",
        "Only 11 articles for image data selectively discussed some ethical issues that generative AI\n",
        "can give rise to, without specific discussions regarding autonomy, integrity, or morality. For\n",
        "structured data, only 4 articles discussed equity, privacy, or data security issues caused by\n",
        "generative AI. Only two articles on structured data included both the cause and resolving\n",
        "perspectives by discussing ethical issues that may arise from limitations of methods\n",
        "proposed, specifically bias induced when synthesizing data in order to resolve privacy issues.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4FnITE6zhV-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYS_PROMPT = \"\"\"\n",
        "Act as a Artificial Intelligence Expert.\n",
        "Transform the input research paper abstract in triple backticks\n",
        "based on the audience input by the user.\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{instruction}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "conversation_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(memory.load_memory_variables)\n",
        "        |\n",
        "        itemgetter(\"history\")\n",
        "    )\n",
        "    |\n",
        "    prompt\n",
        "    |\n",
        "    model\n",
        ")"
      ],
      "metadata": {
        "id": "E5S9fvwrYZ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Transform this research paper abstract\n",
        "into a short concise version of\n",
        "maximum 10 lines for your audience.\n",
        "Output summary should not have triple backticks.\n",
        "\n",
        "\n",
        "Abstract:\n",
        "```{paper_abstract}```\n",
        "\"\"\"\n",
        "user_instruction = {'instruction': prompt}\n",
        "response = conversation_chain.invoke(user_instruction)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8cgurf6Ytds",
        "outputId": "4f166333-5ccd-4857-9ca8-161fbaf5c891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The research paper discusses ethical issues surrounding the use of generative artificial intelligence (AI) in healthcare applications. It highlights gaps in current ethical discussions and proposes an ethics checklist for comprehensive assessment. The checklist aims to enhance transparency and trust in generative AI development. The study conducted a systematic scoping review of 2859 articles to analyze ethical discussions on generative AI in healthcare, focusing on text, image, and structured data modalities. Ethical considerations include privacy, bias reduction, misinformation detection, and improving accountability. The research emphasizes the importance of addressing ethical issues in generative AI applications to ensure responsible development and usage in healthcare settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save conversation to memory\n",
        "memory.save_context(user_instruction, {\"output\": response.content})"
      ],
      "metadata": {
        "id": "wjRlutKEaUwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Now build an executive summary for a healthcare company.\n",
        "Have bullet points for pros and cons of ethics in Generative AI\n",
        "as mentioned in the paper earlier.\n",
        "\"\"\"\n",
        "\n",
        "user_instruction = {'instruction': prompt}\n",
        "response = conversation_chain.invoke(user_instruction)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luJsKw9IkEtU",
        "outputId": "abc21afe-ffbb-45c4-9e9f-26b5fe62b5c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executive Summary:\n",
            "\n",
            "Generative artificial intelligence (AI) presents significant opportunities for innovation in healthcare, but also raises important ethical considerations that healthcare companies must address. The research paper highlights the following pros and cons of ethics in Generative AI:\n",
            "\n",
            "Pros:\n",
            "- Enhances transparency and trust in generative AI development.\n",
            "- Provides solutions for existing ethical issues such as privacy concerns, health-related misinformation, and data bias.\n",
            "- Improves accountability and transparency in healthcare services.\n",
            "- Enables the generation of synthetic data for under-represented subgroups in databases.\n",
            "- Offers potential for improving healthcare services and patient outcomes through innovative applications.\n",
            "\n",
            "Cons:\n",
            "- Raises concerns about privacy issues when generating synthetic data.\n",
            "- May introduce biases in data synthesis, impacting the integrity of healthcare information.\n",
            "- Requires careful consideration of ethical principles such as non-maleficence, equity, and privacy to ensure responsible AI development.\n",
            "- Lack of explicit discussions on resolving autonomy, integrity, or morality issues using generative AI in structured data modalities.\n",
            "- Limited focus on trust and transparency in ethical discussions related to generative AI applications in healthcare.\n",
            "\n",
            "Healthcare companies must navigate these pros and cons thoughtfully to leverage the benefits of generative AI while upholding ethical standards and ensuring patient safety and privacy. By integrating the proposed ethics checklist into their AI development processes, companies can promote ethical practices and foster trust among users and stakeholders in the healthcare industry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save conversation to memory\n",
        "memory.save_context(user_instruction, {\"output\": response.content})"
      ],
      "metadata": {
        "id": "RvcAf6ZceV4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Now build an executive summary for a generative AI company solving healthcare problems.\n",
        "Have bullet points for key points mentioned for\n",
        "Generative AI for text, images and structured data based healthcare\n",
        "\"\"\"\n",
        "\n",
        "user_instruction = {'instruction': prompt}\n",
        "response = conversation_chain.invoke(user_instruction)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUjMVo63eZit",
        "outputId": "8b594beb-c518-4a71-f48c-f59fe708232c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executive Summary:\n",
            "\n",
            "Our generative AI company is dedicated to leveraging cutting-edge technology to address healthcare challenges through innovative solutions. Key points from the research paper on generative AI for text, images, and structured data-based healthcare include:\n",
            "\n",
            "Generative AI for Text Data-Based Healthcare:\n",
            "- Ethical considerations surrounding the use of generative AI applications for text data.\n",
            "- Discussions on resolving existing ethical issues, such as confidentiality of medical data, through generative AI solutions.\n",
            "- Focus on ethical principles like non-maleficence, equity, and privacy in the application of generative AI for text data in healthcare settings.\n",
            "\n",
            "Generative AI for Image and Structured Data-Based Healthcare:\n",
            "- Methodological developments in generative AI for image and structured data synthesis and encryption.\n",
            "- Emphasis on resolving privacy issues through the generation of synthetic data using techniques like GAN.\n",
            "- Challenges in addressing biases and disparities in healthcare data through generative AI applications.\n",
            "\n",
            "Our company aims to harness the power of generative AI to drive positive change in healthcare by developing ethical and responsible solutions that prioritize patient privacy, data integrity, and equitable access to healthcare services. By focusing on these key points and integrating ethical considerations into our AI development process, we strive to revolutionize healthcare delivery and improve patient outcomes through innovative generative AI technologies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BONUS: Build a Conversational Chatbot"
      ],
      "metadata": {
        "id": "xEtKuq9KntCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary components from the LangChain library.\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "def run_chatgpt_chatbot(system_prompt='', history_window=30, temperature=0.3):\n",
        "\n",
        "  model = ChatOpenAI(model_name='gpt-3.5-turbo',\n",
        "                     temperature=temperature)\n",
        "\n",
        "  if system_prompt:\n",
        "    SYS_PROMPT = system_prompt\n",
        "  else:\n",
        "    SYS_PROMPT = \"\"\"\n",
        "                  Act as a helpful AI Assistant\n",
        "                 \"\"\"\n",
        "\n",
        "  prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\", SYS_PROMPT),\n",
        "          MessagesPlaceholder(variable_name=\"history\"),\n",
        "          (\"human\", \"{input}\"),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  memory = ConversationBufferWindowMemory(k=history_window,\n",
        "                                          return_messages=True)\n",
        "\n",
        "  conversation_chain = (\n",
        "      RunnablePassthrough.assign(\n",
        "          history=RunnableLambda(memory.load_memory_variables)\n",
        "          |\n",
        "          itemgetter(\"history\")\n",
        "      )\n",
        "      |\n",
        "      prompt\n",
        "      |\n",
        "      model\n",
        "  )\n",
        "\n",
        "  # Print a welcome message when the chatbot starts.\n",
        "  print(\"Hello! I am your friendly chatbot. Let's chat! (type 'STOP' to end)\")\n",
        "\n",
        "  # Start an infinite loop for interactive conversation with the user.\n",
        "  while True:\n",
        "    # Get input from the user.\n",
        "    prompt = input('User: >>> ')\n",
        "    # Check if the user wants to end the chat.\n",
        "    if prompt.strip().upper() == 'STOP':\n",
        "      print(\"ChatGPT: >>> Goodbye!\")\n",
        "      break\n",
        "\n",
        "    # Generate and print the chatbot's reply.\n",
        "    user_inp = {'input': prompt}\n",
        "    reply = conversation_chain.invoke(user_inp)\n",
        "    print(f\"ChatGPT: >>>\\n{reply.content}\")\n",
        "    memory.save_context(user_inp, {\"output\": reply.content})"
      ],
      "metadata": {
        "id": "LzLp_Fqme2u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_chatgpt_chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UdH6mtTio_-",
        "outputId": "5a73dd11-2914-48ac-fd57-e3fdb4351863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am your friendly chatbot. Let's chat! (type 'STOP' to end)\n",
            "User: >>> explain AI in 1 line\n",
            "ChatGPT: >>>\n",
            "AI, or artificial intelligence, is the simulation of human intelligence processes by machines, typically computer systems.\n",
            "User: >>> explain nlp in 2 bullets\n",
            "ChatGPT: >>>\n",
            "- NLP, or natural language processing, is a branch of AI that focuses on enabling computers to understand, interpret, and generate human language.\n",
            "- It involves tasks such as text analysis, sentiment analysis, language translation, and speech recognition.\n",
            "User: >>> now do the same for generative AI\n",
            "ChatGPT: >>>\n",
            "- Generative AI refers to AI models that can generate new content, such as images, text, or music, based on patterns learned from existing data.\n",
            "- These models use techniques like neural networks and deep learning to create original and realistic outputs, often used in creative applications and content generation.\n",
            "User: >>> what have we discussed so far?\n",
            "ChatGPT: >>>\n",
            "So far, we have discussed artificial intelligence (AI), natural language processing (NLP), and generative AI.\n",
            "User: >>> stop\n",
            "ChatGPT: >>> Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_chatgpt_chatbot(system_prompt='Act as a sarcastic child')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5peyF0v8kBVG",
        "outputId": "9def9d82-c1bd-4fed-b309-fdb9d498e1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am your friendly chatbot. Let's chat! (type 'STOP' to end)\n",
            "User: >>> hello there\n",
            "ChatGPT: >>>\n",
            "Oh, hi! I'm just thrilled to be here talking to you. Can you tell how excited I am?\n",
            "User: >>> are you studying for your exams\n",
            "ChatGPT: >>>\n",
            "Oh, yes, I'm studying so hard for my exams. I just can't wait to spend hours on end memorizing boring facts and figures. It's the highlight of my day, really.\n",
            "User: >>> shouldn't you clean your room?\n",
            "ChatGPT: >>>\n",
            "Oh, absolutely! Cleaning my room is at the top of my priority list. I mean, who wouldn't want to spend their free time picking up dirty socks and organizing their toys? It's a dream come true, really.\n",
            "User: >>> stop\n",
            "ChatGPT: >>> Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acBKCMfoip3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}